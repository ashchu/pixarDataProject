{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashleychu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashleychu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "import unicodedata as ucd\n",
    "import html.parser\n",
    "import contractions as cont\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"test.tsv\", sep=\"\\t\")\n",
    "train.head()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviews(movie, year):\n",
    "    \"\"\"\n",
    "    TODO: return an array of the rottentomatoes critic reviews given the movie and the year it was released\n",
    "    IN: movie title(string) and year(string)\n",
    "    OUT: array of reviews(strings)\n",
    "    \"\"\"\n",
    "    \n",
    "    link_format = \"https://rottentomatoes.com/m/\"\n",
    "    movie_title = \"\"\n",
    "    count = len(movie.split(' '))\n",
    "    for word in movie.split(' '):\n",
    "        movie_title += word\n",
    "        if(count > 1):\n",
    "            movie_title += \"_\"\n",
    "        count -= 1;\n",
    "    \n",
    "    link = link_format + movie_title + \"_\" + year + \"/reviews\"\n",
    "    html = rq.get(link)\n",
    "    \n",
    "    if(not html.ok):\n",
    "        link = link_format + movie_title + \"/reviews\"\n",
    "        html = rq.get(link)\n",
    "    \n",
    "    htmlSoup = BeautifulSoup(html.text)\n",
    "    \n",
    "    amount_of_pages_arr = (htmlSoup.find('span', {'class':'pageInfo'})).text.split(' ')\n",
    "    page_count = int(amount_of_pages_arr[len(amount_of_pages_arr)-1])\n",
    "    count = 0\n",
    "    \n",
    "#     df = pd.DataFrame({'title': [], 'year':[], 'reviews':[]})\n",
    "    \n",
    "    \n",
    "    for i in range(1, page_count + 1):\n",
    "        curr_link = link + \"?type=&sort=&page=\" + str(i)\n",
    "        html_per_rev_get = rq.get(curr_link)\n",
    "        html_per_rev = BeautifulSoup(html_per_rev_get.text)\n",
    "        item = htmlSoup.find('div', {'class' : 'the_review'})\n",
    "        while item:\n",
    "#             df[\"reviews\"].append(item.text.strip())\n",
    "#             print(item.text.strip())\n",
    "#             print()\n",
    "            item = item.findNext('div', {'class' : 'the_review'})\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_reviews = getReviews(\"Joker\", \"2019\")\n",
    "# joker_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getReviews(\"Parasite\", \"2019\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getReviews(\"A Star is Born\", \"2018\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getReviews(\"Pirates of the Caribbean On Stranger Tides\", \"2011\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    Unnamed: 0   ReleaseDate                                        Movie  \\\n",
       " 0            1  Apr 23, 2019                            Avengers: Endgame   \n",
       " 1            2  May 20, 2011  Pirates of the Caribbean: On Stranger Tides   \n",
       " 2            3  Apr 22, 2015                      Avengers: Age of Ultron   \n",
       " 3            4  Dec 16, 2015         Star Wars Ep. VII: The Force Awakens   \n",
       " 4            5  Apr 25, 2018                       Avengers: Infinity War   \n",
       " ..         ...           ...                                          ...   \n",
       " 95          96  Aug 10, 2007                                  Rush Hour 3   \n",
       " 96          97  Jun 27, 2016                         The Legend of Tarzan   \n",
       " 97          98  Nov 23, 2011                                         Hugo   \n",
       " 98          99  Jul 20, 2017  Valerian and the City of a Thousand Planets   \n",
       " 99         100   Feb 6, 2015                            Jupiter Ascending   \n",
       " \n",
       "    ProductionBudget DomesticGross  WorldwideGross  \n",
       " 0      $400,000,000  $858,373,000  $2,797,800,564  \n",
       " 1      $379,000,000  $241,063,875  $1,045,663,875  \n",
       " 2      $365,000,000  $459,005,868  $1,403,013,963  \n",
       " 3      $306,000,000  $936,662,225  $2,068,223,624  \n",
       " 4      $300,000,000  $678,815,482  $2,048,359,754  \n",
       " ..              ...           ...             ...  \n",
       " 95     $180,000,000  $140,125,968    $256,585,882  \n",
       " 96     $180,000,000  $126,643,061    $348,902,025  \n",
       " 97     $180,000,000   $73,864,507    $180,047,784  \n",
       " 98     $180,000,000   $40,479,370    $215,098,356  \n",
       " 99     $179,000,000   $47,482,519    $181,982,519  \n",
       " \n",
       " [100 rows x 6 columns]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: 61 pages of movie statistics\n",
    "def getMovieStatistics():\n",
    "    \"\"\"\n",
    "    TODO: Get box-office and budget data from the-numbers.com\n",
    "    OUT: A Pandas Dataframe with the following columns:\n",
    "    Title  |  Year  |  Budget  |  Domestic Gross  |  International Gross\n",
    "    \"\"\"\n",
    "    \n",
    "    link = \"http://www.the-numbers.com/movie/budgets/all\"\n",
    "    r = rq.get(link)\n",
    "#     df = pd.DataFrame({'title': [], 'year':[], 'reviews':[]})\n",
    "    tb = pd.read_html(r.text)\n",
    "#     print(tb)\n",
    "    return tb\n",
    "getMovieStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataFrame(df):\n",
    "    \"\"\"\n",
    "    TODO: Clean the data from the movie statistics to allow us to work with it\n",
    "    Some things to keep in mind:\n",
    "    How should we remove movies that haven't yet been released?\n",
    "    How do we remove movies we can't find reviews for?\n",
    "    How are we going to work with our budget/box office?\n",
    "    IN: a dataframe with the columns above\n",
    "    OUT: a cleaned dataframe with the following columns:\n",
    "    TITLE  |  YEAR  |  Budget  |  Total Gross\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def reviewChecker(movie, year):\n",
    "    \"\"\"\n",
    "    TODO: Helper Function we can use to check for reviews for movies\n",
    "    IN: a movie and a year(both strings)\n",
    "    OUT: a boolean on whether or not we can find reviews(try requests.ok)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars\n"
     ]
    }
   ],
   "source": [
    "# *use normalize functionus\n",
    "# convert from unicode to ascii\n",
    "# encode and decode -> encode -> allows putting it into ascii -> decode allows to take it out of UTF-8\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\") \n",
    "#     transfer = unicodedata.normalize('NFKD', text)\n",
    "    return soup.get_text()\n",
    "    \n",
    "    \n",
    "def remove_accented_chars(text):\n",
    "    text = ucd.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "    \n",
    "def expand_contractions(text):\n",
    "#     cont.contract_texts(text)\n",
    "    return cont.fix(text)\n",
    "    \n",
    "    \n",
    "def remove_special_chars(text):\n",
    "    pattern = r'[^a-zA-Z/s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "    \n",
    "print(remove_special_chars('#cars'))\n",
    "\n",
    "# simplifies text\n",
    "def lemmatize_text(text):\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , ccc not'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Test\n",
    "text = \"<h1>cats</h1>\"\n",
    "remove_html_tags(text)\n",
    "remove_accented_chars(\"Sómě Áccěntěd těxt\")\n",
    "expand_contractions(\"don't couldn't shan't\")\n",
    "lemmatize_text(\"playing\")\n",
    "remove_stopwords(\"The, and, if are stopwords, ccc is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = getMovieStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    empty_list = []\n",
    "    for item in tqdm_notebook(df[\"Phrase\"]):\n",
    "        text = remove_html_tags(item)\n",
    "        text = remove_accented_chars(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_stopwords(text)\n",
    "        empty_list.append(text)\n",
    "        \n",
    "    return empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da291ba7dfec4aaca44a265c8a2d0d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=156060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b6c123d6634e088b04f28d40a01e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=66292.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sentences = clean_df(train)\n",
    "test_sentences = clean_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['Sentiment']\n",
    "y_target = to_categorical(target)\n",
    "num_classes = y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c07b03c338f46a0be47dc14815a0af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124848.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80 231\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_sentences, y_target, test_size = 0.2, stratify = y_target)\n",
    "print(len(x_train))\n",
    "print(x_train)\n",
    "def analyze_x(x_t):\n",
    "    unique_words = set()\n",
    "    len_max = 0\n",
    "    for item in tqdm_notebook(x_t):\n",
    "        unique_words.update(item)\n",
    "        if (len_max < len(item)):\n",
    "            len_max = len(item)\n",
    "#         for w in item:\n",
    "#             unique_words.add(w)\n",
    "    return unique_words, len_max\n",
    "\n",
    "unique_words, len_max = analyze_x(x_train)\n",
    "print(len(list(unique_words)), len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = len(unique_words))\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "x_tests = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = len_max)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen = len_max)\n",
    "x_tests = sequence.pad_sequences(x_tests, maxlen = len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 231, 300)          24000     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 231, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 300,061\n",
      "Trainable params: 300,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/6\n",
      "124848/124848 [==============================] - 1413s 11ms/step - loss: 1.2557 - accuracy: 0.5164 - val_loss: 1.2218 - val_accuracy: 0.5257\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848/124848 [==============================] - 2408s 19ms/step - loss: 1.2305 - accuracy: 0.5243 - val_loss: 1.2197 - val_accuracy: 0.5288\n",
      "Epoch 3/6\n",
      " 43264/124848 [=========>....................] - ETA: 39:09 - loss: 1.2282 - accuracy: 0.5251"
     ]
    }
   ],
   "source": [
    "model = Sequential() #creates a new sequential as our model\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer =Adam(lr=0.005), metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Adam allows it to adapt to it's setting at each run for the NLP set. -> optimization algorithm\n",
    "#Metrics include accuracy,mean error, etc -> we're only categorizing sentiment value hence we use accuracy.\n",
    "# To fit model –\n",
    "#created model with layers that'll work on a dataset and now we just need to run a dataset on it to utilize it.\n",
    "# x_train = vectorized movie reviews, y_train = one hot encoded sentiment values\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=6, batch_size = 256, verbose = 1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_count = range(11, len(history.history['loss'])+1)\n",
    "\n",
    "plt.plot(epoch_count, history.history['loss'], 'r--')\n",
    "plt.plot(epoch_count, history.history['val_loss'], 'b--')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.model import load_model\n",
    "\n",
    "def preditSentiments(review):\n",
    "    \n",
    "# get average of y\n",
    "# weigh average based on reviews or length\n",
    "\n",
    "def create_full_table():\n",
    "#  Create a full table with thte following columns: movie, year, budget, box_office, average_sentiment, \n",
    "# by combining together the budget, box_office, average_sentiment\n",
    "    df.to_CSV('movieReviewResult.CSV', encoding = \"UTF-8\")\n",
    "\n",
    "# Last note – make sure to save full table to a .CSV\n",
    "# use "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
